{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries for this Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only Numpy Library is necessary for this project and all the layers are compiled with Numpy.\n",
    "import numpy as np \n",
    "import pickle \n",
    "import sys\n",
    "import time\n",
    "import pdb          ### library for debugging\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Convolutional Layer Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "    # Initialization of convolutional layer\n",
    "    def __init__(self, inputs_channel, num_filters, kernel_size, padding, stride, learning_rate, name):\n",
    "        # weight size: (F, C, K, K)\n",
    "        # bias size: (F) \n",
    "        self.F = num_filters\n",
    "        self.K = kernel_size\n",
    "        self.C = inputs_channel\n",
    "\n",
    "        self.weights = np.zeros((self.F, self.C, self.K, self.K))\n",
    "        self.bias = np.zeros((self.F, 1))\n",
    "        for i in range(0,self.F):\n",
    "            self.weights[i,:,:,:] = np.random.normal(loc=0, scale=np.sqrt(1./(self.C*self.K*self.K)), size=(self.C, self.K, self.K))\n",
    "\n",
    "        self.p = padding\n",
    "        self.s = stride\n",
    "        self.lr = learning_rate\n",
    "        self.name = name\n",
    "    \n",
    "    # Padding Layer \n",
    "    def zero_padding(self, inputs, size):\n",
    "        w, h = inputs.shape[0], inputs.shape[1]\n",
    "        new_w = 2 * size + w\n",
    "        new_h = 2 * size + h\n",
    "        out = np.zeros((new_w, new_h))\n",
    "        out[size:w+size, size:h+size] = inputs\n",
    "        return out\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, inputs):\n",
    "        # input size: (C, W, H)\n",
    "        # output size: (N, F ,WW, HH)\n",
    "        C = inputs.shape[0]\n",
    "        W = inputs.shape[1]+2*self.p\n",
    "        H = inputs.shape[2]+2*self.p\n",
    "        self.inputs = np.zeros((C, W, H))\n",
    "        for c in range(inputs.shape[0]):\n",
    "            self.inputs[c,:,:] = self.zero_padding(inputs[c,:,:], self.p)\n",
    "        WW = (W - self.K)//self.s + 1\n",
    "        HH = (H - self.K)//self.s + 1\n",
    "        feature_maps = np.zeros((self.F, WW, HH))\n",
    "        for f in range(self.F):\n",
    "            for w in range(WW):\n",
    "                for h in range(HH):\n",
    "                    feature_maps[f,w,h]=np.sum(self.inputs[:,w:w+self.K,h:h+self.K]*self.weights[f,:,:,:])+self.bias[f]\n",
    "\n",
    "        return feature_maps\n",
    "    \n",
    "    # Backward Propagation\n",
    "    def backward(self, dy):\n",
    "\n",
    "        C, W, H = self.inputs.shape\n",
    "        dx = np.zeros(self.inputs.shape)\n",
    "        dw = np.zeros(self.weights.shape)\n",
    "        db = np.zeros(self.bias.shape)\n",
    "\n",
    "        F, W, H = dy.shape\n",
    "        for f in range(F):\n",
    "            for w in range(W):\n",
    "                for h in range(H):\n",
    "                    dw[f,:,:,:]+=dy[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]\n",
    "                    dx[:,w:w+self.K,h:h+self.K]+=dy[f,w,h]*self.weights[f,:,:,:]\n",
    "\n",
    "        for f in range(F):\n",
    "            db[f] = np.sum(dy[f, :, :])\n",
    "\n",
    "        self.weights -= self.lr * dw\n",
    "        self.bias -= self.lr * db\n",
    "        return dx\n",
    "    \n",
    "    # Function for extract the weights and bias for storage\n",
    "    def extract(self):\n",
    "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
    "    \n",
    "    # Feed the pretrained weights and bias for models \n",
    "    def feed(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: MaxPooling Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxpooling2D:\n",
    "    # Initialization of MaxPooling layer\n",
    "    def __init__(self, pool_size, stride, name):\n",
    "        self.pool = pool_size\n",
    "        self.s = stride\n",
    "        self.name = name\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        C, W, H = inputs.shape\n",
    "        new_width = (W - self.pool)//self.s + 1\n",
    "        new_height = (H - self.pool)//self.s + 1\n",
    "        out = np.zeros((C, new_width, new_height))\n",
    "        for c in range(C):\n",
    "            for w in range(W//self.s):\n",
    "                for h in range(H//self.s):\n",
    "                    out[c, w, h] = np.max(self.inputs[c, w*self.s:w*self.s+self.pool, h*self.s:h*self.s+self.pool])\n",
    "        return out\n",
    "    \n",
    "    # Backward propagation\n",
    "    def backward(self, dy):\n",
    "        C, W, H = self.inputs.shape\n",
    "        dx = np.zeros(self.inputs.shape)\n",
    "        \n",
    "        for c in range(C):\n",
    "            for w in range(0, W, self.pool):\n",
    "                for h in range(0, H, self.pool):\n",
    "                    st = np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
    "                    (idx, idy) = np.unravel_index(st, (self.pool, self.pool))\n",
    "                    dx[c, w+idx, h+idy] = dy[c, w//self.pool, h//self.pool]\n",
    "        return dx\n",
    "    \n",
    "    # No weights and bias for pooling layer to store\n",
    "    def extract(self):\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Fully-Connected Layer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    # Initialization of Fully-Connected Layer\n",
    "    def __init__(self, num_inputs, num_outputs, learning_rate, name):\n",
    "        self.weights = 0.01*np.random.rand(num_inputs, num_outputs)\n",
    "        self.bias = np.zeros((num_outputs, 1))\n",
    "        self.lr = learning_rate\n",
    "        self.name = name\n",
    "    \n",
    "    # Forward Propagation\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.dot(self.inputs, self.weights) + self.bias.T\n",
    "    \n",
    "    # Backward Propagation\n",
    "    def backward(self, dy):\n",
    "\n",
    "        if dy.shape[0] == self.inputs.shape[0]:\n",
    "            dy = dy.T\n",
    "        dw = dy.dot(self.inputs)\n",
    "        db = np.sum(dy, axis=1, keepdims=True)\n",
    "        dx = np.dot(dy.T, self.weights.T)\n",
    "\n",
    "        self.weights -= self.lr * dw.T\n",
    "        self.bias -= self.lr * db\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    # Extract weights and bias for storage\n",
    "    def extract(self):\n",
    "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
    "    \n",
    "    # Feed the pretrained weights and bias for models \n",
    "    def feed(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "### Flatten function to convert 4D feature maps into 3D feature vectors\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        self.C, self.W, self.H = inputs.shape\n",
    "        return inputs.reshape(1, self.C*self.W*self.H)\n",
    "    def backward(self, dy):\n",
    "        return dy.reshape(self.C, self.W, self.H)\n",
    "    def extract(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Activation Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ReLU activation function\n",
    "class ReLu:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        ret = inputs.copy()\n",
    "        ret[ret < 0] = 0\n",
    "        return ret\n",
    "    def backward(self, dy):\n",
    "        dx = dy.copy()\n",
    "        dx[self.inputs < 0] = 0\n",
    "        return dx\n",
    "    def extract(self):\n",
    "        return\n",
    "\n",
    "### Softmax activation function\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        exp = np.exp(inputs, dtype=np.float)\n",
    "        self.out = exp/np.sum(exp)\n",
    "        return self.out\n",
    "    def backward(self, dy):\n",
    "        return self.out.T - dy.reshape(dy.shape[0],1)\n",
    "    def extract(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Loss Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross-Entropy Loss function\n",
    "def cross_entropy(inputs, labels):\n",
    "    out_num = labels.shape[0]\n",
    "    p = np.sum(labels.reshape(1,out_num)*inputs)\n",
    "    loss = -np.log(p)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Neural Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This step shows how to define a simple CNN with all kind of layers which we introduced above.\n",
    "\"\"\"\n",
    "class Net:\n",
    "    def __init__(self):\n",
    "        # input: 28x28\n",
    "        # output: 1x4 (only a subset, containing 4 classes, of the MNIST will be used)\n",
    "        # conv1:  {(28-5+0x0)/2+1} -> (12x12x6) (output size of convolutional layer)\n",
    "        # maxpool2: {(12-2)/2+1} -> (6x6)x6 (output size of pooling layer)\n",
    "        # fc3: 216 -> 32\n",
    "        # fc4: 32 -> 4\n",
    "        # softmax: 4 -> 4\n",
    "        lr = 0.001\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution2D(inputs_channel=1, num_filters=6, kernel_size=5, padding=0, stride=2, learning_rate=lr, name='conv1'))\n",
    "        self.layers.append(ReLu())\n",
    "        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool2'))\n",
    "        self.layers.append(Flatten())\n",
    "        self.layers.append(FullyConnected(num_inputs=6*6*6, num_outputs=32, learning_rate=lr, name='fc3'))\n",
    "        self.layers.append(ReLu())\n",
    "        self.layers.append(FullyConnected(num_inputs=32, num_outputs=4, learning_rate=lr, name='fc4'))\n",
    "        self.layers.append(Softmax())\n",
    "        self.lay_num = len(self.layers)\n",
    "    \n",
    "    ### Function for train the network\n",
    "    def train(self, data, label):\n",
    "        batch_size = data.shape[0]\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        for b in range(batch_size):\n",
    "            x = data[b]\n",
    "            y = label[b]\n",
    "            # forward pass\n",
    "            for l in range(self.lay_num):\n",
    "                output = self.layers[l].forward(x)\n",
    "                x = output\n",
    "            loss += cross_entropy(output, y)\n",
    "            if np.argmax(output) == np.argmax(y):\n",
    "                acc += 1\n",
    "            # backward pass\n",
    "            dy = y\n",
    "            for l in range(self.lay_num-1, -1, -1):\n",
    "                dout = self.layers[l].backward(dy)\n",
    "                dy = dout\n",
    "        return loss, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Create your own subset samples of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'precode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d3ea78980a8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprecode\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \"\"\"\n\u001b[0;32m      3\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msubset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mMNIST\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlast\u001b[0m \u001b[1;36m4\u001b[0m \u001b[0mdigits\u001b[0m \u001b[0mof\u001b[0m \u001b[0myour\u001b[0m \u001b[0mASUID\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThere\u001b[0m \u001b[0mare\u001b[0m \u001b[1;36m4\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msamples\u001b[0m \u001b[0mare\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mshuffled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'precode'"
     ]
    }
   ],
   "source": [
    "from precode import *\n",
    "\"\"\"\n",
    "The subset of MNIST is created based on the last 4 digits of your ASUID. There are 4 categories and all returned \n",
    "samples are preprocessed and shuffled. \n",
    "\"\"\"\n",
    "print('Loading data......')\n",
    "sub_train_images, sub_train_labels, sub_test_images, sub_test_labels = init_subset('5027') # input your ASUID last 4 digits here to generate the subset samples of MNIST for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Initial Network and do the Training and Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "epoch = 10            ### Default number of epochs\n",
    "batch_size = 100      ### Default batch size\n",
    "num_batch = sub_train_images.shape[0]/batch_size\n",
    "\n",
    "test_size = sub_test_images.shape[0]      # Obtain the size of testing samples\n",
    "train_size = sub_train_images.shape[0]    # Obtain the size of training samples\n",
    "\n",
    "epoch_history=[]\n",
    "train_acc_history=[]\n",
    "train_loss_history=[]\n",
    "test_acc_history=[]\n",
    "test_loss_history=[]\n",
    "\n",
    "### ---------- ###\n",
    "\"\"\"\n",
    "Please compile your own evaluation code based on the training code \n",
    "to evaluate the trained network.\n",
    "The function name and the inputs of the function have been predifined and please finish the remaining part.\n",
    "\"\"\"\n",
    "def evaluate(net, images, labels):\n",
    "    acc = 0    \n",
    "    loss = 0\n",
    "    batch_size = 1\n",
    "\n",
    "    pass\n",
    "    for batch_index in range(0, images.shape[0], batch_size):\n",
    "        \"\"\"\n",
    "        Please compile your main code here.\n",
    "        \"\"\"\n",
    "        \n",
    "        if batch_index + batch_size < images.shape[0]:\n",
    "            data = images[batch_index:batch_index+batch_size]\n",
    "            label = labels[batch_index:batch_index + batch_size]\n",
    "        else:\n",
    "            data = images[batch_index:images.shape[0]]\n",
    "            label = labels[batch_index:labels.shape[0]]\n",
    "            \n",
    "        data=data.reshape(data.shape[0], data.shape[2], data.shape[3])\n",
    "        label=label.T\n",
    "        \n",
    "        # forward pass\n",
    "        for l in range(net.lay_num):\n",
    "            output = net.layers[l].forward(data)\n",
    "            data = output\n",
    "\n",
    "        loss += cross_entropy(output, label)\n",
    "        if np.argmax(output) == np.argmax(label):\n",
    "            acc += 1\n",
    "                \n",
    "    return round(acc/images.shape[0], 2), round(loss/images.shape[0], 2)\n",
    "\n",
    "### Start training process\n",
    "for e in range(epoch):\n",
    "    total_acc = 0    \n",
    "    total_loss = 0\n",
    "    print('Epoch %d' % e)\n",
    "    for batch_index in range(0, sub_train_images.shape[0], batch_size):\n",
    "        # batch input\n",
    "        if batch_index + batch_size < sub_train_images.shape[0]:\n",
    "            data = sub_train_images[batch_index:batch_index+batch_size]\n",
    "            label = sub_train_labels[batch_index:batch_index + batch_size]\n",
    "        else:\n",
    "            data = sub_train_images[batch_index:sub_train_images.shape[0]]\n",
    "            label = sub_train_labels[batch_index:sub_train_labels.shape[0]]\n",
    "        # Compute the remaining time\n",
    "        start_time = time.time()\n",
    "        batch_loss,batch_acc = net.train(data, label)  # Train the network with samples in one batch \n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time-start_time\n",
    "        remain_time = (sub_train_images.shape[0]-batch_index)/batch_size*batch_time\n",
    "        hrs = int(remain_time/3600)\n",
    "        mins = int((remain_time/60-hrs*60))\n",
    "        secs = int(remain_time-mins*60-hrs*3600)\n",
    "        print('=== Iter:{0:d} === Remain: {1:d} Hrs {2:d} Mins {3:d} Secs ==='.format(int(batch_index+batch_size),int(hrs),int(mins),int(secs)))\n",
    "    \n",
    "    # Print out the Performance\n",
    "    train_acc, train_loss = evaluate(net, sub_train_images, sub_train_labels)  # Use the evaluation code to obtain the training accuracy and loss\n",
    "    test_acc, test_loss = evaluate(net, sub_test_images, sub_test_labels)      # Use the evaluation code to obtain the testing accuracy and loss\n",
    "    print('=== Epoch:{0:d} Train Size:{1:d}, Train Acc:{2:.3f}, Train Loss:{3:.3f} ==='.format(e, train_size,train_acc,train_loss))\n",
    "\n",
    "    print('=== Epoch:{0:d} Test Size:{1:d}, Test Acc:{2:.3f}, Test Loss:{3:.3f} ==='.format(e, test_size, test_acc,test_loss))\n",
    "    \n",
    "    epoch_history.append(e)\n",
    "    train_acc_history.append(train_acc)\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_acc_history.append(test_acc)\n",
    "    test_loss_history.append(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Train Accuracy: \"+ str(train_acc_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Train Loss: \"+ str(train_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Test Accuracy: \"+ str(test_acc_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Test Loss: \"+ str(test_loss_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_history, train_acc_history)\n",
    "plt.title(\"Epoch vs Train Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_history, train_loss_history)\n",
    "plt.title(\"Epoch vs Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_history, test_acc_history)\n",
    "plt.title(\"Epoch vs Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_history, test_loss_history)\n",
    "plt.title(\"Epoch vs Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplot(2,2)\n",
    "axs[0,0].plot(epoch_history, train_acc_history)\n",
    "axs[0,0].set_title(\"Epoch vs Train Accuracy\")\n",
    "axs[0,1].plot(epoch_history, train_loss_history)\n",
    "axs[0,1].set_title(\"Epoch vs Train Loss\")\n",
    "axs[1,0].plot(epoch_history, test_acc_history)\n",
    "axs[1,0].set_title(\"Epoch vs Test Accuracy\")\n",
    "axs[1,1].plot(epoch_history, test_loss_history)\n",
    "axs[1,1].set_title(\"Epoch vs Test Loss\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
